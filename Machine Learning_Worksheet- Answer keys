Answer keys of ML_worksheet

Q.1
Ans: C
Q.2
Ans: C
Q.3
Ans: C
Q.4
Ans: A
Q.5
Ans: B
Q.6
Ans: D
Q.7
Ans: B
Q.8
Ans: D
Q.9
Ans: A,B,D
Q.10
Ans: A,B,D


Q.11
Ans:

An outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analysis.

IQR is the range between the first and the third quartiles namely Q1 and Q3: IQR = Q3 – Q1. Interquartile range (IQR) is the difference between the 75th percentile (0.75 quantile) and the 25th percentile (0.25 quantile).
Practical use of the IQR is to detect outliers in your data. The general rule is that outliers are observations that fall:

below 25th percentile – 1.5 * IQR, or
above 75th percentile + 1.5 * IQR


Q.12
Ans:

Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data.

Boosting is an iterative technique which adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation. Boosting in general builds strong predictive models.

Q.13
Ans:

The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not.  It is always lower than the R-squared.

R-squared measures the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model. Adjusted R-squared adjusts the statistic based on the number of independent variables in the model.R-squared shows how well terms (data points) fit a curve or line. Adjusted R-squared also indicates how well terms fit a curve or line, but adjusts for the number of terms in a model. If you add more and more useless variables to a model, adjusted r-squared will decrease. If you add more useful variables, adjusted r-squared will increase.

Adjusted R-squared will always be less than or equal to R-squared. 

Formula for  calculate Adjusted R-squared

Adjusted R Squared = 1 – [((1 – R**2) * (n – 1)) / (n – k – 1)]


Q.14
Ans:

The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1.


Q.15
Ans:

Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. Later, you test your model on this sample before finalizing it.

Advantage of cross validation-

Reduces Overfitting: In Cross Validation, we split the dataset into multiple folds and train the algorithm on different folds. This prevents our model from overfitting the training dataset. So, in this way, the model attains the generalization capabilities which is a good sign of a robust algorithm.

Disadvantage of cross validation-

Increases Training Time: Cross Validation drastically increases the training time. Earlier you had to train your model only on one training set, but with Cross Validation you have to train your model on multiple training sets. 
